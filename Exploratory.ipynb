{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cleaned_train_with_features.csv')\n",
    "X, y = df.drop('TARGET',axis=1), df['TARGET']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation Splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stratified Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified Split\n",
    "def stratified_split(data, test_size=0.1, validation_size = 0.1):\n",
    "\n",
    "    class_1 = data[data['TARGET'] == 1]\n",
    "    class_0 = data[data['TARGET'] == 0]\n",
    "\n",
    "    test_count_1 = int(len(class_1) * test_size)\n",
    "    validation_count_1 = int(len(class_1) * validation_size)\n",
    "    test_count_0 = int(len(class_0) * test_size)\n",
    "    validation_count_0 = int(len(class_0) * validation_size)\n",
    "\n",
    "    class_1 = class_1.sample(frac=1)\n",
    "    class_0 = class_0.sample(frac=1)\n",
    "\n",
    "    # Split each class into test, validation, and train sets\n",
    "    test_data = pd.concat([class_1.iloc[:test_count_1], class_0.iloc[:test_count_0]])\n",
    "    validation_data = pd.concat([\n",
    "        class_1.iloc[test_count_1:test_count_1 + validation_count_1],\n",
    "        class_0.iloc[test_count_0:test_count_0 + validation_count_0]\n",
    "    ])\n",
    "    train_data = pd.concat([\n",
    "        class_1.iloc[test_count_1 + validation_count_1:],\n",
    "        class_0.iloc[test_count_0 + validation_count_0:]\n",
    "    ])\n",
    "\n",
    "    # Split features and target for each set\n",
    "    X_train = train_data.drop('TARGET',axis=1)\n",
    "    X_test = test_data.drop('TARGET',axis=1)\n",
    "    X_val = validation_data.drop('TARGET',axis=1)\n",
    "    y_train = train_data['TARGET']\n",
    "    y_test = test_data['TARGET']\n",
    "    y_val = validation_data['TARGET']\n",
    "\n",
    "    return X_train, X_test, X_val, y_train, y_test, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of 187047    1\n",
       "82891     1\n",
       "39875     1\n",
       "212962    1\n",
       "13540     1\n",
       "         ..\n",
       "255389    0\n",
       "214840    0\n",
       "136169    0\n",
       "218754    0\n",
       "139408    0\n",
       "Name: TARGET, Length: 26341, dtype: int64>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, X_val, y_train, y_test, y_val = stratified_split(df)\n",
    "y_val.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Split\n",
    "\n",
    "def custom_split(data, valid_prop = 0.1, test_prop = 0.1, random_seed = 1738):\n",
    "\n",
    "    train_prop = 1 - valid_prop - test_prop\n",
    "\n",
    "    # define bins for age and income\n",
    "    age_bins = [0, 40, 60, np.inf]\n",
    "    age_labels = ['young', 'middle_aged', 'senior']\n",
    "    income_bins = [0, 30000, 70000, np.inf]\n",
    "    income_labels = ['low', 'medium', 'high']\n",
    "\n",
    "    # convert age to years\n",
    "    data['age'] = data['DAYS_BIRTH']/-365\n",
    "\n",
    "    # create binned variables\n",
    "    data['age_group'] = pd.cut(data['age'], bins=age_bins, labels=age_labels)\n",
    "    data['income_group'] = pd.cut(data['AMT_INCOME_TOTAL'], bins=income_bins, labels=income_labels)\n",
    "\n",
    "    # create a key for each group (combination of gender, age, and income)\n",
    "    data['group_key'] = data['CODE_GENDER_M'].astype(str) + '_' + data['age_group'].astype(str) + '_' + data['income_group'].astype(str)\n",
    "\n",
    "    # shuffle the data\n",
    "    data = data.sample(frac=1, random_state=random_seed).reset_index(drop=True)\n",
    "\n",
    "    # split the data based on key\n",
    "    train_data = pd.DataFrame()\n",
    "    val_data = pd.DataFrame()\n",
    "    test_data = pd.DataFrame()\n",
    "\n",
    "    for key, group in data.group_by('group_key'):\n",
    "        n = len(group)\n",
    "        n_train = int(n * train_prop)\n",
    "        n_val = int(n * valid_prop)\n",
    "\n",
    "        train_data = pd.concat([train_data, group[:n_train]])\n",
    "        val_data = pd.concat([val_data, group[n_train:n_train + n_val]])\n",
    "        test_data = pd.concat([test_data, group[n_train + n_val:]])\n",
    "\n",
    "    # drop all unnecesary columns\n",
    "    train_data = train_data.drop(columns=['age','age_group', 'income_group', 'strat_key'])\n",
    "    val_data = val_data.drop(columns=['age','age_group', 'income_group', 'strat_key'])\n",
    "    test_data = test_data.drop(columns=['age','age_group', 'income_group', 'strat_key'])\n",
    "\n",
    "    X_train = train_data.drop(['TARGET'])\n",
    "    y_train = train_data['TARGET']\n",
    "\n",
    "    X_validation = val_data.drop(['TARGET'])\n",
    "    y_validation = val_data['TARGET']\n",
    "\n",
    "    X_test = test_data.drop(['TARGET'])\n",
    "    y_test = test_data['TARGET']\n",
    "\n",
    "    return X_train, y_train, X_validation, y_validation, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_train_test_split(X, y, test_size=0.2, val_size=0.2, random_seed=42):\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    n = len(X)\n",
    "    indices = np.arange(n)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    test_indices = indices[:int(n * test_size)]\n",
    "    val_indices = indices[int(n * test_size):int(n * (test_size + val_size))]\n",
    "    train_indices = indices[int(n * (test_size + val_size)):]\n",
    "\n",
    "    X_train, y_train = X[train_indices], y[train_indices]\n",
    "    X_test, y_test = X[test_indices], y[test_indices]\n",
    "    X_val, y_val = X[val_indices], y[val_indices]\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_true, y_pred):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    FN = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    \n",
    "    return float(TP / (TP + FN)) if (TP + FN) > 0 else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_true, y_pred):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    FP = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    \n",
    "    return float(TP / (TP + FP)) if (TP + FP) > 0 else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "def compute_accuracy(truth, predicted):\n",
    "    return np.mean(truth == predicted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 Score\n",
    "def calculate_f1(y_true, y_pred):\n",
    "    TP = FP = TN = FN = 0\n",
    "\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        if true == 1 and pred == 1:\n",
    "            TP += 1\n",
    "        elif true == 0 and pred == 1:\n",
    "            FP += 1\n",
    "        elif true == 0 and pred == 0:\n",
    "            TN += 1\n",
    "        elif true == 1 and pred == 0:\n",
    "            FN += 1  ,\n",
    "\n",
    "    precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
    "    recall = TP / (TP + FN) if TP + FN > 0 else 0\n",
    "    if precision + recall == 0:\n",
    "        return 0\n",
    "\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F2 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F2 Score\n",
    "def calculate_f2(y_true, y_pred, b=2):\n",
    "    r = recall(y_true, y_pred)\n",
    "    p = precision(y_true, y_pred)\n",
    "    \n",
    "    if r + p == 0:\n",
    "        return 0\n",
    "    \n",
    "    f2 = (1 + b**2) * (r * p) / (b**2 * r + p)\n",
    "    return f2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC-AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_roc_auc(y_true, y_prob):\n",
    "  sorted_indices = np.argsort(y_prob)\n",
    "  true_sort = y_true[sorted_indices]\n",
    "  prob_sort = y_prob[sorted_indices]\n",
    "\n",
    "  TP = np.cumsum(true_sort)\n",
    "  FP = np.cumsum(1 - true_sort)\n",
    "\n",
    "  TPR = TP / TP[-1]\n",
    "  FPR = FP / FP[-1]\n",
    "\n",
    "  # rocauc\n",
    "  rocauc_score = np.trapz(FPR, TPR)\n",
    "\n",
    "  return rocauc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intercept: -4.9781205980785475e-08 \n",
      " weights: [-6.29774349e-08 -4.64958120e-07  3.17404965e-06  1.99527078e-06\n",
      " -4.00132795e-06 -1.84860477e-09  1.04920090e-04  1.05758003e-06\n",
      "  2.94474735e-05  1.39724412e-04 -4.97812060e-08 -4.91318944e-08\n",
      "  9.25746059e-09 -4.93129808e-08 -2.56061234e-08 -9.29348728e-09\n",
      " -1.52258031e-07 -3.94736910e-08 -3.64593397e-08 -8.33245052e-07\n",
      " -8.92672183e-10 -1.08029531e-09 -1.29193443e-09  8.22923853e-09\n",
      "  1.26363114e-08  8.27006751e-09 -9.45893716e-08 -1.87506555e-08\n",
      "  2.75656894e-08 -1.81917093e-08  2.18276236e-08  1.04428263e-04\n",
      "  0.00000000e+00  9.02088395e-09 -6.15921339e-11 -7.57262878e-10\n",
      "  1.32004091e-09 -2.59283591e-11 -1.51248000e-08 -9.91530921e-10\n",
      " -1.72214747e-11 -1.54882650e-09 -3.32840780e-12 -2.30770593e-09\n",
      " -2.02809573e-09 -6.46480071e-10 -4.27045635e-09 -2.69147229e-10\n",
      " -3.94890033e-09 -6.66345010e-11  7.64050565e-11 -6.45403092e-11\n",
      " -2.76156807e-10  5.58503023e-10 -1.05099974e-09 -2.88165937e-08\n",
      " -1.29116991e-08  5.88900257e-08 -3.77847825e-08  3.79159188e-08\n",
      " -5.17454954e-08 -2.92201811e-08 -9.85411391e-09 -1.44324389e-10\n",
      " -4.07642704e-10 -3.50492101e-10 -3.64976002e-09 -3.50116100e-08\n",
      " -3.43339300e-08 -6.69661823e-12 -6.76549674e-10 -2.28721602e-08\n",
      " -5.95030753e-11 -6.57296820e-12  8.18060454e-09 -8.27675889e-08\n",
      " -1.14483870e-08  3.52295930e-09  4.12183136e-08 -4.37104446e-08\n",
      "  2.41344109e-09 -1.45601275e-08  1.86487052e-09 -4.27347982e-08\n",
      "  1.69235899e-09 -2.37754829e-09  1.28016671e-09 -6.56729694e-09\n",
      " -1.27281196e-08 -9.13195902e-09 -5.50254337e-09 -6.53194408e-09\n",
      " -5.12226455e-09 -6.10776910e-09  7.17274824e-10 -5.47710521e-09\n",
      " -1.63693119e-09 -1.95705454e-09 -2.78825273e-10  3.05349658e-11\n",
      "  6.05045635e-09 -4.23610154e-10 -7.46379782e-10 -4.32927173e-10\n",
      " -6.31409123e-09 -4.35656340e-10  1.41440879e-10  7.83954805e-10\n",
      " -4.68589681e-11 -3.61238052e-10 -6.84585643e-10 -5.14864124e-12\n",
      " -4.15454490e-10  1.65187522e-09  1.29300152e-11 -4.99606960e-10\n",
      " -8.51137662e-11 -3.27117929e-10 -2.36546569e-11 -2.76467902e-09\n",
      " -5.92889576e-10 -5.82917578e-09 -7.70716400e-11 -7.19466501e-09\n",
      " -4.13311249e-09 -2.86673022e-10 -5.21879534e-09 -4.25609177e-09\n",
      " -3.09460484e-10  2.93048902e-10 -1.41747547e-11  1.10308114e-09\n",
      " -7.89701136e-09  1.44337278e-09 -3.48980203e-09  9.31344268e-09\n",
      " -1.57673240e-09 -4.15054548e-10 -3.74271349e-10 -3.71696096e-09\n",
      " -9.06532943e-10 -1.51849263e-10 -5.79039344e-11 -9.33974764e-10\n",
      " -3.33254716e-10 -3.48872633e-10 -1.58078443e-09  2.65566901e-09\n",
      "  5.30053754e-10 -1.17844306e-09 -6.69997506e-10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sreshtalluri/Documents/CSC/DATA/data403-project2/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# logistic regression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X, y)\n",
    "w = lr.coef_[0]\n",
    "b = lr.intercept_[0]\n",
    "print(f'intercept: {b} \\n weights: {w}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression W/ Penalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intercept: -4.9781205980785475e-08 \n",
      " weights: [-6.29774349e-08 -4.64958120e-07  3.17404965e-06  1.99527078e-06\n",
      " -4.00132795e-06 -1.84860477e-09  1.04920090e-04  1.05758003e-06\n",
      "  2.94474735e-05  1.39724412e-04 -4.97812060e-08 -4.91318944e-08\n",
      "  9.25746059e-09 -4.93129808e-08 -2.56061234e-08 -9.29348728e-09\n",
      " -1.52258031e-07 -3.94736910e-08 -3.64593397e-08 -8.33245052e-07\n",
      " -8.92672183e-10 -1.08029531e-09 -1.29193443e-09  8.22923853e-09\n",
      "  1.26363114e-08  8.27006751e-09 -9.45893716e-08 -1.87506555e-08\n",
      "  2.75656894e-08 -1.81917093e-08  2.18276236e-08  1.04428263e-04\n",
      "  0.00000000e+00  9.02088395e-09 -6.15921339e-11 -7.57262878e-10\n",
      "  1.32004091e-09 -2.59283591e-11 -1.51248000e-08 -9.91530921e-10\n",
      " -1.72214747e-11 -1.54882650e-09 -3.32840780e-12 -2.30770593e-09\n",
      " -2.02809573e-09 -6.46480071e-10 -4.27045635e-09 -2.69147229e-10\n",
      " -3.94890033e-09 -6.66345010e-11  7.64050565e-11 -6.45403092e-11\n",
      " -2.76156807e-10  5.58503023e-10 -1.05099974e-09 -2.88165937e-08\n",
      " -1.29116991e-08  5.88900257e-08 -3.77847825e-08  3.79159188e-08\n",
      " -5.17454954e-08 -2.92201811e-08 -9.85411391e-09 -1.44324389e-10\n",
      " -4.07642704e-10 -3.50492101e-10 -3.64976002e-09 -3.50116100e-08\n",
      " -3.43339300e-08 -6.69661823e-12 -6.76549674e-10 -2.28721602e-08\n",
      " -5.95030753e-11 -6.57296820e-12  8.18060454e-09 -8.27675889e-08\n",
      " -1.14483870e-08  3.52295930e-09  4.12183136e-08 -4.37104446e-08\n",
      "  2.41344109e-09 -1.45601275e-08  1.86487052e-09 -4.27347982e-08\n",
      "  1.69235899e-09 -2.37754829e-09  1.28016671e-09 -6.56729694e-09\n",
      " -1.27281196e-08 -9.13195902e-09 -5.50254337e-09 -6.53194408e-09\n",
      " -5.12226455e-09 -6.10776910e-09  7.17274824e-10 -5.47710521e-09\n",
      " -1.63693119e-09 -1.95705454e-09 -2.78825273e-10  3.05349658e-11\n",
      "  6.05045635e-09 -4.23610154e-10 -7.46379782e-10 -4.32927173e-10\n",
      " -6.31409123e-09 -4.35656340e-10  1.41440879e-10  7.83954805e-10\n",
      " -4.68589681e-11 -3.61238052e-10 -6.84585643e-10 -5.14864124e-12\n",
      " -4.15454490e-10  1.65187522e-09  1.29300152e-11 -4.99606960e-10\n",
      " -8.51137662e-11 -3.27117929e-10 -2.36546569e-11 -2.76467902e-09\n",
      " -5.92889576e-10 -5.82917578e-09 -7.70716400e-11 -7.19466501e-09\n",
      " -4.13311249e-09 -2.86673022e-10 -5.21879534e-09 -4.25609177e-09\n",
      " -3.09460484e-10  2.93048902e-10 -1.41747547e-11  1.10308114e-09\n",
      " -7.89701136e-09  1.44337278e-09 -3.48980203e-09  9.31344268e-09\n",
      " -1.57673240e-09 -4.15054548e-10 -3.74271349e-10 -3.71696096e-09\n",
      " -9.06532943e-10 -1.51849263e-10 -5.79039344e-11 -9.33974764e-10\n",
      " -3.33254716e-10 -3.48872633e-10 -1.58078443e-09  2.65566901e-09\n",
      "  5.30053754e-10 -1.17844306e-09 -6.69997506e-10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sreshtalluri/Documents/CSC/DATA/data403-project2/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# scale the data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# logistic regression w penalty\n",
    "lr_penalty = LogisticRegression(penalty='l2', C=0.1)\n",
    "lr_penalty.fit(X, y)\n",
    "w = lr_penalty.coef_[0]\n",
    "b = lr_penalty.intercept_[0]\n",
    "print(f'intercept: {b} \\n weights: {w}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC Model\n",
    "svc_model = SVC(gamma='auto', kernel='linear') #Can add standard scaler if you want to scale, but left it out to interpret results\n",
    "svc_model.fit(X, y)\n",
    "weights = svc_model.coef_\n",
    "b = svc_model.intercept_\n",
    "print(f'weights: {weights} \\n constant: {b}')\n",
    "print(f'Actual decision boundary: {-b/weights}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: [[-6.99163540e-03  1.31726591e-07  2.51570304e-06  7.90656431e-06\n",
      "  -2.97870729e-06  3.30432518e+00  3.10964090e-05  6.09622973e-05\n",
      "   1.22852234e-05  6.92994698e-05 -5.49536286e-14 -1.13795177e+00\n",
      "   2.32766005e-01 -1.04844708e-01 -6.25567688e-02 -3.06116334e-02\n",
      "   1.04697501e-02 -1.36067968e-01  3.02887942e-01  2.76813800e-03\n",
      "  -2.55896995e-01  1.49182311e-01 -1.99011053e-01  3.04084734e-01\n",
      "   3.83162041e-03  3.11606065e-02 -2.58909916e+00  2.51117890e-02\n",
      "   1.66423113e-01 -2.74171808e-02  6.51061324e-02  9.02112208e-05\n",
      "  -1.89972739e-13  5.84966274e-01 -2.68696131e-01  5.20468085e-01\n",
      "   5.29385595e-01  1.62834989e-01  3.62887028e-01  3.65029890e-01\n",
      "  -1.06538072e+00 -1.13021294e-01  2.83997546e-01 -4.60871568e-01\n",
      "  -5.49975143e-01 -5.08995001e-01 -4.66557311e-01 -1.02500534e+00\n",
      "  -4.28563348e-01 -2.19961398e-01  5.62758668e-01  4.62208840e-01\n",
      "  -2.76730968e-02  1.31825353e-01 -1.53816093e-02 -3.96546600e-03\n",
      "  -1.66288010e-02  3.37600839e-02  2.41319189e-01  4.22802912e-01\n",
      "  -2.71682156e-01  3.72548838e-04 -3.27834494e-02 -2.24157469e-01\n",
      "  -5.34511656e-02  3.86173708e-02 -8.46201299e-02  2.10753113e-02\n",
      "   2.96658991e-02 -6.01450587e-02 -4.42241813e-01  5.05865108e-02\n",
      "  -1.18541995e+00 -3.00947038e-01  1.53792523e-01  4.42972954e-01\n",
      "   4.73662056e-01  8.67542799e-01  7.10562373e-01 -1.59479085e-01\n",
      "  -1.20447940e-02 -4.43692451e-02 -1.26470995e-01  1.18785132e-01\n",
      "   2.45754623e-01 -9.06002748e-02  2.47219648e-01  2.18883097e-01\n",
      "  -6.00143086e-02 -7.46093873e-02 -9.40538718e-02 -2.38605352e-02\n",
      "  -1.54403658e-02 -1.25651313e-02 -2.99350689e-01 -4.95165451e-01\n",
      "  -1.81870526e-01 -1.87049672e-01 -1.38665845e-01 -2.81532948e-01\n",
      "   2.72276546e-02 -3.60055594e-01 -3.59344741e-01 -3.96084679e-01\n",
      "  -3.02613478e-01 -3.96225270e-01 -2.18321638e-01  3.10836395e-02\n",
      "  -2.94680970e-01 -2.09649838e-01 -7.58390646e-01 -6.43667102e-01\n",
      "  -4.65268256e-01 -2.99013648e-02 -1.96743672e-01 -4.49693397e-01\n",
      "  -3.37576419e-01 -2.24354119e-01 -8.01347152e-01 -4.83766233e-01\n",
      "  -2.16050887e-01 -3.10661475e-01  1.03839471e-01 -2.64440062e-01\n",
      "  -6.00865880e-01 -1.16239708e-01 -2.48543538e-01 -5.51743043e-01\n",
      "  -1.56037193e-01  3.58076522e-01 -2.67016440e-01  4.01603486e-02\n",
      "  -3.40661402e-01 -2.47436162e-01 -5.42199221e-01 -9.88403825e-02\n",
      "  -3.44238657e-01 -1.94980529e-01 -3.96156827e-01 -4.83060007e-01\n",
      "  -5.67086747e-02 -8.77571154e-01 -5.48768344e-01 -5.94454834e-01\n",
      "  -1.74941374e-01 -6.92572781e-01 -3.38738070e-01  5.51900799e-01\n",
      "  -1.79126238e-01 -2.73529436e-01 -2.33242150e+01]] \n",
      " constant: [-0.96448307]\n"
     ]
    }
   ],
   "source": [
    "# LDA Model\n",
    "\n",
    "lda_model = LinearDiscriminantAnalysis()\n",
    "\n",
    "lda_model.fit(X,y)\n",
    "\n",
    "weights = lda_model.coef_\n",
    "b = lda_model.intercept_\n",
    "\n",
    "print(f'weights: {weights} \\n constant: {b}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
