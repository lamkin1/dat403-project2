{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cleaned_train_with_features.csv')\n",
    "X, y = df.drop('TARGET',axis=1), df['TARGET']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation Splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stratified Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified Split\n",
    "def stratified_split(data, test_size=0.1, validation_size = 0.1):\n",
    "\n",
    "    class_1 = data[data['TARGET'] == 1]\n",
    "    class_0 = data[data['TARGET'] == 0]\n",
    "\n",
    "    test_count_1 = int(len(class_1) * test_size)\n",
    "    validation_count_1 = int(len(class_1) * validation_size)\n",
    "    test_count_0 = int(len(class_0) * test_size)\n",
    "    validation_count_0 = int(len(class_0) * validation_size)\n",
    "\n",
    "    class_1 = class_1.sample(frac=1)\n",
    "    class_0 = class_0.sample(frac=1)\n",
    "\n",
    "    # Split each class into test, validation, and train sets\n",
    "    test_data = pd.concat([class_1.iloc[:test_count_1], class_0.iloc[:test_count_0]])\n",
    "    validation_data = pd.concat([\n",
    "        class_1.iloc[test_count_1:test_count_1 + validation_count_1],\n",
    "        class_0.iloc[test_count_0:test_count_0 + validation_count_0]\n",
    "    ])\n",
    "    train_data = pd.concat([\n",
    "        class_1.iloc[test_count_1 + validation_count_1:],\n",
    "        class_0.iloc[test_count_0 + validation_count_0:]\n",
    "    ])\n",
    "\n",
    "    # Split features and target for each set\n",
    "    X_train = train_data.drop('TARGET',axis=1)\n",
    "    X_test = test_data.drop('TARGET',axis=1)\n",
    "    X_val = validation_data.drop('TARGET',axis=1)\n",
    "    y_train = train_data['TARGET']\n",
    "    y_test = test_data['TARGET']\n",
    "    y_val = validation_data['TARGET']\n",
    "\n",
    "    return X_train, X_test, X_val, y_train, y_test, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of 187047    1\n",
       "82891     1\n",
       "39875     1\n",
       "212962    1\n",
       "13540     1\n",
       "         ..\n",
       "255389    0\n",
       "214840    0\n",
       "136169    0\n",
       "218754    0\n",
       "139408    0\n",
       "Name: TARGET, Length: 26341, dtype: int64>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, X_val, y_train, y_test, y_val = stratified_split(df)\n",
    "y_val.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Split\n",
    "\n",
    "def custom_split(data, valid_prop = 0.1, test_prop = 0.1, random_seed = 1738):\n",
    "\n",
    "    train_prop = 1 - valid_prop - test_prop\n",
    "\n",
    "    # define bins for age and income\n",
    "    age_bins = [0, 40, 60, np.inf]\n",
    "    age_labels = ['young', 'middle_aged', 'senior']\n",
    "    income_bins = [0, 30000, 70000, np.inf]\n",
    "    income_labels = ['low', 'medium', 'high']\n",
    "\n",
    "    # convert age to years\n",
    "    data['age'] = data['DAYS_BIRTH']/-365\n",
    "\n",
    "    # create binned variables\n",
    "    data['age_group'] = pd.cut(data['age'], bins=age_bins, labels=age_labels)\n",
    "    data['income_group'] = pd.cut(data['AMT_INCOME_TOTAL'], bins=income_bins, labels=income_labels)\n",
    "\n",
    "    # create a key for each group (combination of gender, age, and income)\n",
    "    data['group_key'] = data['CODE_GENDER_M'].astype(str) + '_' + data['age_group'].astype(str) + '_' + data['income_group'].astype(str)\n",
    "\n",
    "    # shuffle the data\n",
    "    data = data.sample(frac=1, random_state=random_seed).reset_index(drop=True)\n",
    "\n",
    "    # split the data based on key\n",
    "    train_data = pd.DataFrame()\n",
    "    val_data = pd.DataFrame()\n",
    "    test_data = pd.DataFrame()\n",
    "\n",
    "    for key, group in data.group_by('group_key'):\n",
    "        n = len(group)\n",
    "        n_train = int(n * train_prop)\n",
    "        n_val = int(n * valid_prop)\n",
    "\n",
    "        train_data = pd.concat([train_data, group[:n_train]])\n",
    "        val_data = pd.concat([val_data, group[n_train:n_train + n_val]])\n",
    "        test_data = pd.concat([test_data, group[n_train + n_val:]])\n",
    "\n",
    "    # drop all unnecesary columns\n",
    "    train_data = train_data.drop(columns=['age','age_group', 'income_group', 'strat_key'])\n",
    "    val_data = val_data.drop(columns=['age','age_group', 'income_group', 'strat_key'])\n",
    "    test_data = test_data.drop(columns=['age','age_group', 'income_group', 'strat_key'])\n",
    "\n",
    "    X_train = train_data.drop(['TARGET'])\n",
    "    y_train = train_data['TARGET']\n",
    "\n",
    "    X_validation = val_data.drop(['TARGET'])\n",
    "    y_validation = val_data['TARGET']\n",
    "\n",
    "    X_test = test_data.drop(['TARGET'])\n",
    "    y_test = test_data['TARGET']\n",
    "\n",
    "    return X_train, y_train, X_validation, y_validation, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_train_test_split(X, y, test_size=0.2, val_size=0.2, random_seed=42):\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    n = len(X)\n",
    "    indices = np.arange(n)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    test_indices = indices[:int(n * test_size)]\n",
    "    val_indices = indices[int(n * test_size):int(n * (test_size + val_size))]\n",
    "    train_indices = indices[int(n * (test_size + val_size)):]\n",
    "\n",
    "    X_train, y_train = X[train_indices], y[train_indices]\n",
    "    X_test, y_test = X[test_indices], y[test_indices]\n",
    "    X_val, y_val = X[val_indices], y[val_indices]\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_true, y_pred):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    FN = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    \n",
    "    return float(TP / (TP + FN)) if (TP + FN) > 0 else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_true, y_pred):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    FP = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    \n",
    "    return float(TP / (TP + FP)) if (TP + FP) > 0 else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "def compute_accuracy(truth, predicted):\n",
    "    return np.mean(truth == predicted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 Score\n",
    "def calculate_f1(y_true, y_pred):\n",
    "    TP = FP = TN = FN = 0\n",
    "\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        if true == 1 and pred == 1:\n",
    "            TP += 1\n",
    "        elif true == 0 and pred == 1:\n",
    "            FP += 1\n",
    "        elif true == 0 and pred == 0:\n",
    "            TN += 1\n",
    "        elif true == 1 and pred == 0:\n",
    "            FN += 1  ,\n",
    "\n",
    "    precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
    "    recall = TP / (TP + FN) if TP + FN > 0 else 0\n",
    "    if precision + recall == 0:\n",
    "        return 0\n",
    "\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F2 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F2 Score\n",
    "def calculate_f2(y_true, y_pred, b=2):\n",
    "    r = recall(y_true, y_pred)\n",
    "    p = precision(y_true, y_pred)\n",
    "    \n",
    "    if r + p == 0:\n",
    "        return 0\n",
    "    \n",
    "    f2 = (1 + b**2) * (r * p) / (b**2 * r + p)\n",
    "    return f2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC-AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_roc_auc(y_true, y_prob):\n",
    "  sorted_indices = np.argsort(y_prob)\n",
    "  true_sort = y_true[sorted_indices]\n",
    "  prob_sort = y_prob[sorted_indices]\n",
    "\n",
    "  TP = np.cumsum(true_sort)\n",
    "  FP = np.cumsum(1 - true_sort)\n",
    "\n",
    "  TPR = TP / TP[-1]\n",
    "  FPR = FP / FP[-1]\n",
    "\n",
    "  # rocauc\n",
    "  rocauc_score = np.trapz(FPR, TPR)\n",
    "\n",
    "  return rocauc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fairness Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 'young'): 0.0019064124783362219, (0, 'middle_aged'): 0.0, (0, 'senior'): 0.0, (1, 'young'): 0.00962544465369324, (1, 'middle_aged'): 0.0034782608695652175, (1, 'senior'): 0.0} \n",
      " 0.004031227890366538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gp/z2gcqhcd7g7d08fq9d3_9d_h0000gn/T/ipykernel_82592/1108828043.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"truth\"] = truth.values\n",
      "/var/folders/gp/z2gcqhcd7g7d08fq9d3_9d_h0000gn/T/ipykernel_82592/1108828043.py:9: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  for key,group in df.groupby(['CODE_GENDER_M', 'AGE_GROUP']):\n",
      "/var/folders/gp/z2gcqhcd7g7d08fq9d3_9d_h0000gn/T/ipykernel_82592/1108828043.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"truth\"] = truth.values\n",
      "/var/folders/gp/z2gcqhcd7g7d08fq9d3_9d_h0000gn/T/ipykernel_82592/1108828043.py:9: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  for key,group in df.groupby(['CODE_GENDER_M', 'AGE_GROUP']):\n"
     ]
    }
   ],
   "source": [
    "# Equal Opportunity\n",
    "def equal_opportunity(classes,truth,preds):\n",
    "    df = classes\n",
    "    df[\"truth\"] = truth.values\n",
    "    df_preds = pd.DataFrame(preds, columns=[\"preds\"])\n",
    "    df = df.join(df_preds)\n",
    "    scores = {}\n",
    "\n",
    "    for key,group in df.groupby(['CODE_GENDER_M', 'AGE_GROUP']):\n",
    "        scores[key] = recall(group['truth'], group['preds'])\n",
    "\n",
    "    values = list(scores.values())\n",
    "    diffs = []\n",
    "\n",
    "    for pair in itertools.combinations(values, 2):\n",
    "        diffs.append(abs(pair[0] - pair[1]))\n",
    "\n",
    "    average_difference = sum(diffs) / len(diffs)\n",
    "\n",
    "\n",
    "    return scores, average_difference\n",
    "\n",
    "# testing\n",
    "age_bins = [0, 40, 60, np.inf]\n",
    "age_labels = ['young', 'middle_aged', 'senior']\n",
    "\n",
    "df['age'] = df['DAYS_BIRTH']/-365\n",
    "\n",
    "df['AGE_GROUP'] = pd.cut(df['age'], bins=age_bins, labels=age_labels)\n",
    "\n",
    "ex_classes = df[['CODE_GENDER_M', 'AGE_GROUP']]\n",
    "\n",
    "preds = lda_model.predict(X)\n",
    "\n",
    "#df_preds = pd.DataFrame(preds, columns=[\"preds\"])\n",
    "#classes.join(df_preds)\n",
    "print(equal_opportunity(ex_classes,y,preds)[0], \"\\n\", equal_opportunity(ex_classes,y,preds)[1])\n",
    "\n",
    "#unique_values, counts = np.unique(preds, return_counts=True)\n",
    "#print(unique_values)\n",
    "#print(counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intercept: -4.8759403511165345e-08 \n",
      " weights: [-6.17098035e-08 -5.11738321e-07  3.24970556e-06  2.17314844e-06\n",
      " -4.08774984e-06 -1.80579918e-09  1.05377936e-04  1.04870586e-06\n",
      "  2.85868825e-05  1.38798174e-04 -4.87594035e-08 -4.81245275e-08\n",
      "  8.92690052e-09 -4.83026440e-08 -2.50491797e-08 -9.06442081e-09\n",
      " -1.49171184e-07 -3.90096871e-08 -3.60710241e-08 -8.14982765e-07\n",
      " -8.63196704e-10 -1.03894884e-09 -1.25067933e-09  8.02450020e-09\n",
      "  1.22751072e-08  8.01566297e-09 -9.23579637e-08 -1.86947865e-08\n",
      "  2.68449653e-08 -1.81464868e-08  2.12597000e-08  1.02539475e-04\n",
      "  0.00000000e+00  8.61561471e-09 -6.00593641e-11 -7.43621294e-10\n",
      "  1.28446036e-09 -2.53485871e-11 -1.47495631e-08 -9.65573225e-10\n",
      " -1.67996890e-11 -1.50942330e-09 -3.25387655e-12 -2.24982981e-09\n",
      " -1.97631805e-09 -6.30331622e-10 -4.16666562e-09 -2.62366014e-10\n",
      " -3.85106962e-09 -6.50571215e-11  7.44024470e-11 -6.26550472e-11\n",
      " -2.69802639e-10  5.43492859e-10 -1.03080323e-09 -2.81655884e-08\n",
      " -1.26380721e-08  5.69097132e-08 -3.68803724e-08  3.69023731e-08\n",
      " -5.05256481e-08 -2.86052845e-08 -9.63592043e-09 -1.40939395e-10\n",
      " -3.97764604e-10 -3.42295529e-10 -3.56721607e-09 -3.43183542e-08\n",
      " -3.34874428e-08 -6.52346950e-12 -6.61456378e-10 -2.23275610e-08\n",
      " -5.80268532e-11 -6.41290073e-12  7.79425991e-09 -8.07334239e-08\n",
      " -1.11579692e-08  3.42983511e-09  4.00010177e-08 -4.27792144e-08\n",
      "  2.33662095e-09 -1.42043739e-08  1.81032685e-09 -4.18726956e-08\n",
      "  1.64424547e-09 -2.32014070e-09  1.24640198e-09 -6.40929455e-09\n",
      " -1.24473175e-08 -8.93271129e-09 -5.37907096e-09 -6.40160812e-09\n",
      " -5.02986173e-09 -5.99406990e-09  6.95360338e-10 -5.33963884e-09\n",
      " -1.60024123e-09 -1.92149638e-09 -3.07020272e-10  2.96289655e-11\n",
      "  5.89729276e-09 -4.13403206e-10 -7.29082773e-10 -4.23128018e-10\n",
      " -6.17128387e-09 -4.25962805e-10  1.34508256e-10  7.63769621e-10\n",
      " -4.57752124e-11 -3.55749360e-10 -6.67599455e-10 -5.17159088e-12\n",
      " -4.05468152e-10  1.60604680e-09  1.19887866e-11 -4.87525242e-10\n",
      " -8.31674905e-11 -3.20395010e-10 -2.30304201e-11 -2.69998714e-09\n",
      " -5.78444621e-10 -5.69944945e-09 -7.50984201e-11 -7.03045701e-09\n",
      " -4.03248095e-09 -2.79218759e-10 -5.10510580e-09 -4.15301108e-09\n",
      " -3.05750623e-10  2.85769816e-10 -1.39013774e-11  1.07567369e-09\n",
      " -7.71296810e-09  1.40348642e-09 -3.40583456e-09  9.04790086e-09\n",
      " -1.53809686e-09 -4.05258051e-10 -3.65067812e-10 -3.62118699e-09\n",
      " -8.85141794e-10 -1.48016365e-10 -5.64573891e-11 -9.11069960e-10\n",
      " -3.29619752e-10 -3.40236759e-10 -1.54402756e-09  2.58885714e-09\n",
      "  5.14174186e-10 -1.15013425e-09 -6.55025962e-10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# logistic regression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X, y)\n",
    "w = lr.coef_[0]\n",
    "b = lr.intercept_[0]\n",
    "print(f'intercept: {b} \\n weights: {w}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression W/ Penalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intercept: -4.9781205980785475e-08 \n",
      " weights: [-6.29774349e-08 -4.64958120e-07  3.17404965e-06  1.99527078e-06\n",
      " -4.00132795e-06 -1.84860477e-09  1.04920090e-04  1.05758003e-06\n",
      "  2.94474735e-05  1.39724412e-04 -4.97812060e-08 -4.91318944e-08\n",
      "  9.25746059e-09 -4.93129808e-08 -2.56061234e-08 -9.29348728e-09\n",
      " -1.52258031e-07 -3.94736910e-08 -3.64593397e-08 -8.33245052e-07\n",
      " -8.92672183e-10 -1.08029531e-09 -1.29193443e-09  8.22923853e-09\n",
      "  1.26363114e-08  8.27006751e-09 -9.45893716e-08 -1.87506555e-08\n",
      "  2.75656894e-08 -1.81917093e-08  2.18276236e-08  1.04428263e-04\n",
      "  0.00000000e+00  9.02088395e-09 -6.15921339e-11 -7.57262878e-10\n",
      "  1.32004091e-09 -2.59283591e-11 -1.51248000e-08 -9.91530921e-10\n",
      " -1.72214747e-11 -1.54882650e-09 -3.32840780e-12 -2.30770593e-09\n",
      " -2.02809573e-09 -6.46480071e-10 -4.27045635e-09 -2.69147229e-10\n",
      " -3.94890033e-09 -6.66345010e-11  7.64050565e-11 -6.45403092e-11\n",
      " -2.76156807e-10  5.58503023e-10 -1.05099974e-09 -2.88165937e-08\n",
      " -1.29116991e-08  5.88900257e-08 -3.77847825e-08  3.79159188e-08\n",
      " -5.17454954e-08 -2.92201811e-08 -9.85411391e-09 -1.44324389e-10\n",
      " -4.07642704e-10 -3.50492101e-10 -3.64976002e-09 -3.50116100e-08\n",
      " -3.43339300e-08 -6.69661823e-12 -6.76549674e-10 -2.28721602e-08\n",
      " -5.95030753e-11 -6.57296820e-12  8.18060454e-09 -8.27675889e-08\n",
      " -1.14483870e-08  3.52295930e-09  4.12183136e-08 -4.37104446e-08\n",
      "  2.41344109e-09 -1.45601275e-08  1.86487052e-09 -4.27347982e-08\n",
      "  1.69235899e-09 -2.37754829e-09  1.28016671e-09 -6.56729694e-09\n",
      " -1.27281196e-08 -9.13195902e-09 -5.50254337e-09 -6.53194408e-09\n",
      " -5.12226455e-09 -6.10776910e-09  7.17274824e-10 -5.47710521e-09\n",
      " -1.63693119e-09 -1.95705454e-09 -2.78825273e-10  3.05349658e-11\n",
      "  6.05045635e-09 -4.23610154e-10 -7.46379782e-10 -4.32927173e-10\n",
      " -6.31409123e-09 -4.35656340e-10  1.41440879e-10  7.83954805e-10\n",
      " -4.68589681e-11 -3.61238052e-10 -6.84585643e-10 -5.14864124e-12\n",
      " -4.15454490e-10  1.65187522e-09  1.29300152e-11 -4.99606960e-10\n",
      " -8.51137662e-11 -3.27117929e-10 -2.36546569e-11 -2.76467902e-09\n",
      " -5.92889576e-10 -5.82917578e-09 -7.70716400e-11 -7.19466501e-09\n",
      " -4.13311249e-09 -2.86673022e-10 -5.21879534e-09 -4.25609177e-09\n",
      " -3.09460484e-10  2.93048902e-10 -1.41747547e-11  1.10308114e-09\n",
      " -7.89701136e-09  1.44337278e-09 -3.48980203e-09  9.31344268e-09\n",
      " -1.57673240e-09 -4.15054548e-10 -3.74271349e-10 -3.71696096e-09\n",
      " -9.06532943e-10 -1.51849263e-10 -5.79039344e-11 -9.33974764e-10\n",
      " -3.33254716e-10 -3.48872633e-10 -1.58078443e-09  2.65566901e-09\n",
      "  5.30053754e-10 -1.17844306e-09 -6.69997506e-10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sreshtalluri/Documents/CSC/DATA/data403-project2/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# scale the data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# logistic regression w penalty\n",
    "lr_penalty = LogisticRegression(penalty='l2', C=0.1)\n",
    "lr_penalty.fit(X, y)\n",
    "w = lr_penalty.coef_[0]\n",
    "b = lr_penalty.intercept_[0]\n",
    "print(f'intercept: {b} \\n weights: {w}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC Model\n",
    "svc_model = SVC(gamma='auto', kernel='linear') #Can add standard scaler if you want to scale, but left it out to interpret results\n",
    "svc_model.fit(X, y)\n",
    "weights = svc_model.coef_\n",
    "b = svc_model.intercept_\n",
    "print(f'weights: {weights} \\n constant: {b}')\n",
    "print(f'Actual decision boundary: {-b/weights}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "[194410  16327]\n",
      "[0 1]\n",
      "[24301  2040]\n",
      "[0 1]\n",
      "[26328    13]\n"
     ]
    }
   ],
   "source": [
    "# LDA Model\n",
    "#print(df)\n",
    "lda_model = LinearDiscriminantAnalysis()\n",
    "\n",
    "X_train, X_test, X_val, y_train, y_test, y_val = stratified_split(df)\n",
    "\n",
    "\n",
    "lda_model.fit(X_train,y_train)\n",
    "\n",
    "preds = lda_model.predict(X_test)\n",
    "\n",
    "unique_values, counts = np.unique(y_train, return_counts=True)\n",
    "print(unique_values)\n",
    "print(counts)\n",
    "\n",
    "unique_values, counts = np.unique(y_test, return_counts=True)\n",
    "print(unique_values)\n",
    "print(counts)\n",
    "\n",
    "\n",
    "unique_values, counts = np.unique(preds, return_counts=True)\n",
    "print(unique_values)\n",
    "print(counts)\n",
    "\n",
    "#print(f'weights: {weights} \\n constant: {b}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
